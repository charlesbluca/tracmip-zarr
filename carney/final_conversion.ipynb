{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `zarr` Conversion and Uploading\n",
    "\n",
    "With our TRACMIP datasets uploaded to the GCS bucket, now we can download the files from there directly to our Lamont machine to perform the conversion. This can be accomplished with a simple long running command:\n",
    "\n",
    "```\n",
    "gsutil -m cp -r gs://pangeo-data/tracmip_temp/* /d3/charles/tracmip/\n",
    "```\n",
    "\n",
    "It is recommended to run this process in the background using either `bg` or `nohup`, as it **will** take hours. With the data downloaded, we can now work to convert and upload each _individual variable_; this is done to prevent cut down on computational time used to merge the different grids of each variable, and will work to prevent grid size conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zarr\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this process will take a long time, so we keep track of each variable's conversion status using a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"converted.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "for path in d:\n",
    "    if not d[path]:\n",
    "        \n",
    "        # get GCS path\n",
    "        time     = path.split(\"/\")[3]\n",
    "        exp      = path.split(\"/\")[4]\n",
    "        model    = path.split(\"/\")[5]\n",
    "        variable = path.split(\"/\")[7].split(\"_\")[0]\n",
    "        version  = path.split(\"/\")[6]\n",
    "        gcs_path = \"gs://pangeo-data/tracmip/%s/%s/%s/%s/%s/\" % (time, exp, model, variable, version)\n",
    "        \n",
    "        print(gcs_path)\n",
    "                \n",
    "        # open dataset\n",
    "        ds = xr.open_dataset(path, chunks={})\n",
    "        ds[variable] = ds[variable].chunk({\"time\" : \"auto\"})\n",
    "        \n",
    "        # convert to zarr\n",
    "        compressor = zarr.Blosc(cname='zstd', clevel=3, shuffle=2)\n",
    "        encoding = {var: {'compressor': compressor} for var in ds.data_vars}\n",
    "        ds.to_zarr(\"zarr/\", mode=\"w\", encoding=encoding, consolidated=True)\n",
    "        \n",
    "        # upload to bucket\n",
    "        !/home/charles/google-cloud-sdk/bin/gsutil -m cp -r zarr/* zarr/.z* {gcs_path}\n",
    "        \n",
    "        # mark as uploaded\n",
    "        d[path] = True\n",
    "        with open(\"converted.json\", \"w\") as f:\n",
    "            json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRACMIP",
   "language": "python",
   "name": "tracmip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
