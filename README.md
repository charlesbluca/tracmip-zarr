# tracmip-zarr
This Github repo will serve as documentation for the process of converting the TRACMIP netCDF4 datasets stored on a server at the University of Miami to the `zarr` format, and then send these datasets to the cloud bucket made available at Pangeo.
From the ground up, this includes:

* Installing Python via Miniconda
* Installing Google Cloud SDK
* Establishing an environment to access and convert datasets
* Connecting to Pangeo's cloud bucket to upload data

First, we want to get multiple versions of Python up and running.
Miniconda 3 will do the trick, and allows us to more easily control the environment we are using:

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sh Miniconda3-latest-Linux-x86_64.sh
```

After installing, we immediately want to set up an environment to install and use Google Cloud SDK in; it will only work with Python 2.7.x:

```bash
conda create -n gcloud python=2.7
conda activate gcloud
```

Now in our newly created environment, we want to get the [latest versioned archive](https://cloud.google.com/sdk/docs/downloads-versioned-archives) of Google Cloud SDK:

```bash
wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-235.0.0-linux-x86_64.tar.gz
tar -xvzf google-cloud-sdk-235.0.0-linux-x86_64.tar.gz
sh google-cloud-sdk/install.sh
```

To make sure the `gcloud` command line interface will work in or out of our environement, we append the following to our `.bashrc`:

```bash
echo "export CLOUDSDK_PYTHON=/home/tracmip/miniconda3/envs/gcloud/bin/python2.7" >> .bashrc
```

Now that we have installed of Python and Google Cloud SDK, we are ready to set up an environment to access our data:

```bash
conda create -n tracmip-zarr python
conda activate tracmip-zarr
```

Once we're in the environment, we want to install the most recent development versions of`xarray` and `zarr`, available on Github:

```bash
pip install git+https://github.com/pydata/xarray.git
pip install git+https://github.com/zarr-developers/zarr.git
```

With these packages taken care of, we install the remainder of our requirements using `conda`:

```bash
conda install netCDF4 dask jupyterlab
```

Optionally, we install `dask-labextension`, which allows for a more intuitive view of our `dask` distributed cluster:

```bash
conda install -c conda-forge nodejs
pip install dask_labextension
jupyter labextension install dask-labextension
```

Once the packages are installed, we are ready to enter the JupyterLab environment and begin interacting with data:

```bash
jupyter lab --port=%port%
```

In order to access the URL generated by Jupyter, we must open another instance of the terminal and redirect this port to our local machine using the `ssh -L` command:

```bash
ssh -L %port%:localhost:%port% tracmip@weather.rsmas.miami.edu
```

Where `%port%` can be filled in with any specified listening port.
